experiment:
    debug: False
    random_seeds: 304
    trainer: EG3DMaskEditTrainer
    exp_name: eg3d_cats_eyes
    log_dir: "%>> /project/dataset/users/liam.1234/localnerf_logs/logs/%(experiment.exp_name)s"
    generated_dir: "evaluation/generated"
    resume_folder: ""

    sample_interval: 30
    model_save_interval: 2000

wandb:
    entity: 'lenerf'
    wandb_run_name: 'default'


model:
    generator:
        model_type: afhqcats512-128
        
        ## Mixing options
        mix_layers: 'first' # 'first' 'last' 'both'
        texture_only: False

    masknet:
        masknet_type: "attention" #"ws_condition"
        use_enlarged_mask: True
        use_coor_input: True
        class_name: "training.clipedit.masknet.Mask_attention"
        hidden_dim: 128
        #thres: 0.                                               # Masknet threshold during eval mode (mask = mask > thres)
    mapper:
        ## VanillaMapper
        #===============#
        #class_name: "training.clipedit.mapper.VanillaMapper"
        #num_layers: 4
        #hidden_dim: 512
        #===============#

        ## StyleCLIP LevelsMapper
        #===============#
        class_name: "training.clipedit.mapper.LevelsMapper"
        num_layers: 4
        no_coarse_mapper: False
        no_medium_mapper: False
        no_fine_mapper: False
        #===============#
    deform:
        class_name: "training.clipedit.deform.Deform_mlp"
        hidden_dim: 128

dataset:
    class_name: "training.dataset.LabelOnlyDataset"
    path: "/project/dataset/users/liam.1234/lenerf/afhq/dataset.json"
lambda:
    masknet_cliploss_lambda: 0.
    cliploss_lambda: 0.1

    mask2dreg: 0.6
    mask2d_minmax_reg: 0.5
    mask2d_tv_reg: 1
    mask2d_topk: 100
    mask2d_botk: 100

    deform_reg_lambda: 0.0001

    lpips_lambda: 0.            # turn off for debugging
    mask_lambda: 0.5
    ws_delta_lambda: 1.
    ws_reg_lambda: 0.002
    
    arcface_lambda: 0.
    delta_arcface_lambda: 0.
    clip_augmentation_num: 4    # how many times to augment?
    sim_scale: 1                # following styleclip. 근데 맞나??

    masked_cliploss: False

training:
    max_step: 50000
    batch_size: 2
    multiview_num: 4
    n_workers: 16
    lr: 0.003
    betas: # eg3d 학습에는 0,0.99 / styleCLIP의 경우 0.95,0.999. 뭐 써야하나
        - 0.95
        - 0.999
    weight_decay: 0
    loss_type: dir_nce # loss type: 'default', 'dir_nce', 'dir_diff'

prompt:
    mask_prompt: ["cat eyes"]
    #edit_prompt: ["a person with rainbow hair", "a photo of a person with rainbow hair", "rainbow hair"]
    #edit_prompt: ["rainbow", "rainbow hair", "rainbow color", "red green orange yellow blue hair"]
    edit_prompt: ["large eyes", "cat with large eyes"]
    src_prompt: ["a photo of a cat", "cat"]
