experiment:
    debug: False
    random_seeds: 304
    trainer: EG3DMaskEditTrainer
    exp_name: eg3d_ffhq_clipedit_face
    log_dir: "%>> /project/dataset/users/liam.1234/localnerf_logs/logs/%(experiment.exp_name)s"
    generated_dir: "evaluation/generated"
    resume_folder: ""

    sample_interval: 30
    model_save_interval: 2000

wandb:
    entity: 'lenerf'
    wandb_run_name: 'default'


model:
    generator:
        model_type: ffhq512-128
        
        ## Mixing options
        mix_layers: 'first' # 'first' 'last' 'both'
        texture_only: False

    masknet:
        masknet_type: "attention" #"ws_condition"
        use_enlarged_mask: True
        use_coor_input: True
        class_name: "training.clipedit.masknet.Mask_attention"
        hidden_dim: 128
        #thres: 0.                                               # Masknet threshold during eval mode (mask = mask > thres)
    mapper:
        #mapper_type: ec
        ## VanillaMapper
        #===============#
        #class_name: "training.clipedit.mapper.VanillaMapper"
        #num_layers: 4
        #hidden_dim: 512
        #===============#

        ## StyleCLIP LevelsMapper
        #===============#
        class_name: "training.clipedit.mapper.LevelsMapper"
        num_layers: 4
        no_coarse_mapper: False
        no_medium_mapper: False
        no_fine_mapper: False
        #===============#

dataset:
    class_name: "training.dataset.LabelOnlyDataset"
    path: "/project/dataset/users/liam.1234/lenerf/dataset.json"
lambda:
    masknet_cliploss_lambda: 0.
    cliploss_lambda: 0.1

    mask2dreg: 0.3
    mask2d_minmax_reg: 0.1
    mask2d_tv_reg: 10
    mask2d_topk: 2000
    mask2d_botk: 100

    lpips_lambda: 0.            # turn off for debugging
    mask_lambda: 0.5
    ws_delta_lambda: 2
    ws_reg_lambda: 0.00001
    arcface_lambda: 0.00001         
    delta_arcface_lambda: 0.00001
    clip_augmentation_num: 4    # how many times to augment?
    sim_scale: 1                # following styleclip. 근데 맞나??

training:
    max_step: 50000
    batch_size: 2
    multiview_num: 4
    n_workers: 16
    lr: 0.003
    betas: # eg3d 학습에는 0,0.99 / styleCLIP의 경우 0.95,0.999. 뭐 써야하나
        - 0.95
        - 0.999
    weight_decay: 0
    loss_type: dir_nce # loss type: 'default', 'dir_nce', 'dir_diff'

prompt:
    mask_prompt: ["face"]
    edit_prompt: ["person with wrinkles on face", "wrinkles"]
    src_prompt: ["a photo of a person", "a photo of a face", "face of a person", "face image", "face"]
